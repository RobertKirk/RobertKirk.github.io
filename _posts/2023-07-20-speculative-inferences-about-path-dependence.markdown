---
layout:  post
title:   "Speculative inferences about path dependence in LLM supervised fine-tuning from results on linear mode connectivity and model souping"
date:    2023-07-20 10:00:00 +0000
tags:    alignment safety
excerpt: I claim that supervised fine-tuning of the existing largest LLMs is likely path-dependent (different random seeds and initialisations have an impact on final performance and model behaviour), based on the fact that when fine-tuning smaller LLMs, models pretrained closer to convergence produce fine-tuned models with similar mechanisms while this isnâ€™t the case for models pretrained without being close to convergence; this is analogous to current LLMs that are very far from convergence at the end of training. This is supported by linking together existing work on model souping, linear mode connectivity, mechanistic similarity and path dependence.
---

This is just a link-post to [this post](https://www.alignmentforum.org/posts/rYdRiaA3cuioJxmBv/speculative-inferences-about-path-dependence-in-llm), which I published on the [Alignment Forum](https://www.alignmentforum.org/).
