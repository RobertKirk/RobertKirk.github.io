---
layout:  post
title:   "Causal confusion as an argument against the scaling hypothesis"
date:    2022-06-20 10:00:00 +0000
tags:    alignment safety
excerpt: We discuss the possibility that causal confusion will be a significant alignment and/or capabilities limitation for current approaches based on "the scaling paradigm": unsupervised offline training of increasingly large neural nets with empirical risk minimization on a large diverse dataset. In particular, this approach may produce a model which uses unreliable (“spurious”) correlations to make predictions, and so fails on “out-of-distribution” data taken from situations where these correlations don’t exist or are reversed. We argue that such failures are particularly likely to be problematic for alignment and/or safety in the case when a system trained to do prediction is then applied in a control or decision-making setting
---

This is just a link-post to [this post](https://www.alignmentforum.org/posts/FZL4ftXvcuKmmobmj/causal-confusion-as-an-argument-against-the-scaling), which I published on the [Alignment Forum](https://www.alignmentforum.org/) with [David Krueger](https://www.davidscottkrueger.com/).
