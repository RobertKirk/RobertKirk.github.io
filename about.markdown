---
layout: page
title: About
permalink: /about/
---

<div>
<p>
<img src="/assets/profile-picture.jpg" alt="me" style="width:  50%; margin-left: 25%; margin-right: 25%; ">
</p>
</div>

I'm Robert Kirk, a software engineer, independent AI researcher and an aspiring <a href="https://www.effectivealtruism.org/">effective altruist</a> and rationalist.

I currently work for <a href="https://smarkets.com/careers/">Smarkets</a>, a sports and politics betting exchange, as an infrastructure and software engineer. I've worked there since August 2018. For the first year and a half I worked as a software engineering in a team developing python micro-services, and then I moved to join the infrastructure team.

Before that I studied for four years at [Somerville college](https://www.some.ox.ac.uk/), University of Oxford, for an integrated masters in mathematics and computer science. Up until the fourth year I maintained a 50/50 split between the two subjects, and in the fourth year I focused more on abstract computer science. I did my [masters thesis](https://github.com/RobertKirk/Graph-Comonads-from-Pebble-Games) on implementing the comonadic formulation of pebble games in [Idris](https://www.idris-lang.org/) under [Samson Abramsky](http://www.cs.ox.ac.uk/people/samson.abramsky/)].

Apart from my day job, I'm interested in AI and AI safety, and how can I use my career to have a positive impact on society. I think the threat of existential risk from AI is real, and think studying that problem is one of the best ways (and the way I'm currently pursuing) to have a positive impact on the future of humanity. I'm hoping to pursue a PhD in machine learning starting autumn 2020. Within the area of machine learning, I'm interested in reinforcement learning, meta learning, natural language processing, interpretability and deep learning (and all the combinations thereof).

I write for the [Alignment Newsletter](https://rohinshah.com/alignment-newsletter/), mostly on the topic of interpretability. Originally formed as part of [AI Safety Research Program](https://aisrp.org/), I'm collaborating with a small group of fellow independent researchers on interpretability, both doing technical research and thinking about how interpretability helps AI alignment.

Other than technical things, I'm interested in rationality, neuroscience, cooking, videogames, music (basically any genre), and reading. I sing in a choir and previously played the French horn.

This site is a place where I'll share my thoughts on all of these topics and possibly more, but the content will mostly be technical AI and AI safety related.
