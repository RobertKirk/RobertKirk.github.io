---
layout: post
title: Introduction to the Vanilla Policy Gradient algorithm
tags: reinforcement_learning pytorch tutorial vpg
---

Today I'm going to present a tutorial on the Vanilla Policy Gradient algorithm, or VPG for short. This is one of the simplest reinforcement learning algorithms, and hence it's easy to understand and a good starting point for anyone interested in deep reinforcement learning.

I'll be demonstrating everything with code, and all the code is available at this github repository [link here]. The code will [Pytorch](https://pytorch.org/ "Pytorch Homepage"), a python deep learning library backed by Facebook which is the main competitor to [Tensorflow](https://www.tensorflow.org/ "Tensorflow Homepage") in the deep learning space. I find Pytorch easier to use, as it's more pythonic, has eager execution built in, the style is close to the normal programming I do (as opposed to Tensorflow, which has a style of declarative programming which I find unintuitive).

I'll give a brief introduction to reinforcement learning and kinds of problems it tries to solve. I'll then describe some key points about the algorithm. We'll see what a simple implementation looks like. Finally, we'll see how we can improve the implementation to get better performance in a number of ways.

# Introduction to Reinforcement Learning #

Reinforcement Learning (henceforth RL) is a machine learning paradigm about learning from trial and error. The key mathematical object is the Markov Decision Process (MDP). This represents the environment the model we train will make decisions in, as well as the rewards it gets for taking those actions. A classic example is the cartpole environment often used in benchmarking reinforcement learning algorithms. The MDP consists of:

* The state space $$ \mathcal{S} $$. In the cartpole example, this is represented by the positions of the cart, the angle of the pole, and velocity of the pole and cart, and their accelerations.
* The action space $$ \mathcal{A} $$. In the cartpole environment, this is just a discrete space with two options: Move Cart Left or Move Cart Right.
* The transition function $$ P: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \Rightarrow [0,1] $$. (Here $$[0,1]$$ is all numbers between 0 and 1 inclusive). This represents the probability function $$P(s \mid a,s)$$ of the next state given the previous state and action. In the most general case this is a stochastic function (and so the next state is random to a certain extend, even given the previous state and action). In the cartpole case however (as in many other environments), the transition function is deterministic, and can then be written as a function $$ P: \mathcal{S} \times \mathcal{A} \Rightarrow \mathcal{S} $$. The cartpole transition function will effectively be a physical model of the system, which will calculate how the cart and pole will move given the current state of the system and how the agent decided to act.
* The initial state distribution $$ s_0 : \mathcal{S} \Rightarrow [0,1] $$. This determines what the starting state of the environment. Again in the general case this is a probability distribution, but for cartpole it's always the same state, so we can just call $$s_0 \in \mathcal{S}$$ the initial state.
* The reward function $$ R: \mathcal{S} \times \mathcal{R} \Rightarrow \mathbb{R} $$ where $$\mathbb{R}$$ is the set of all real numbers. This tells us how much reward we get if we're a certain state and get a certain action.

MDPs often also have a time horizon $$H \in \mathbb{N}$$ where $$\mathbb{N}$$ is the set of all natural numbers (1, 2, 3, etc.). This means that after acting for $$H$$ time steps, the episode finishes.

RL is then the problem of creating an agent (often also called a policy) which acts in this environment by receiving the state, performing an action, receiving the next state and reward, and iterating in this loop until the episode has ended. We want our agent to receive the most reward it can. We can define the reward received in a single episode as $$\sum_{t=0}^{\infty}R(s_t, a_t) $$. Here $$s_t$$ is the state at timestep $$t$$, and $$a_t$$ is the action taken by our agent at timestep $$t$$, in response to receiving the state $$s_t$$. We then want to maximise this reward over all possible episodes generated by the MDP and the agent acting in it.

This is just a brief refresher of the basics of RL. If you haven't encountered it before and want to learn more, I'd recommend the tutorial in [Spinning Up in Deep RL](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html "Spinning Up in Deep RL"), a resource put together by OpenAI.

# The Algorithm #

Now we've covered the basic problem we want to solve, lets get to the good stuff: how we're going to solve it. Any algorithm will have to produce an agent $$\pi$$ which takes a state $$s$$ and outputs an action $$a$$, or a distribution over actions. In the first case we'd say the policy was deterministic (as it'll always pick the same action given the same input state), whereas in the second case we'd call it stochastic. There's many different algorithms which tackle the RL problem, but today I'm presenting one of the simplest. Vanilla Policy Gradient, also called REINFORCE, has been around quite a while. Some key facts:
* The policy is represented by a neural network. This network will take as input the vector representing the state space, and output a vector of probabilities for each action in the action space.
* It's an on policy algorithm. This means it collects data by running it's policy in the environment, and uses this data to update the policy, and then throws this data away. This is called on-policy as the policy is only update with data which it produced, whereas off-policy algorithms will update the policy with data produced by running other policies.
* It's a policy gradient algorithm. This means we learn a parametrised policy $$\pi_{\theta}$$ directly, by using a gradient based method to update the policy.
* It produces a stochastic policy, so given a state $$s$$, $$\pi(s)$$ will be a distribution over actions in $$\mathcal{A}$$

The crucial mathematical result it relies on is call the policy gradient theorem, which I won't prove here, but it effectively guarantees that our policy will eventually converge to the true policy.

The algorithm goes like this:
1. initialise our policy $$\pi_\theta$$
2. Run $$\pi_\theta$$ in the environment, collecting state transitions $$(s_t, a_t, s_{t+1}, r_t, d_t)$$ where $$r_t$$ is the reward at timestep $$t$$, and $$d_t$$ represents whether the episode has terminated or not.
3. calculate the loss function $$J(
